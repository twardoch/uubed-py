{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming API for Large Datasets\n",
    "\n",
    "This notebook demonstrates how to use uubed's streaming API to efficiently process large datasets without loading everything into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from uubed import encode_stream, encode_file_stream, StreamingEncoder, batch_encode\n",
    "import tempfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Streaming from Generators\n",
    "\n",
    "Process embeddings from a generator without loading all data into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_generator(n_embeddings=10000, dimensions=768):\n",
    "    \"\"\"Generate random embeddings on-the-fly.\"\"\"\n",
    "    for i in range(n_embeddings):\n",
    "        # Simulate loading embeddings from a database or API\n",
    "        yield np.random.randint(0, 256, dimensions, dtype=np.uint8)\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Generated {i} embeddings...\", end=\"\\r\")\n",
    "\n",
    "# Process embeddings in a streaming fashion\n",
    "print(\"Processing embeddings with streaming API...\")\n",
    "encoded_count = 0\n",
    "for encoded in encode_stream(embedding_generator(1000, 768), method=\"shq64\"):\n",
    "    encoded_count += 1\n",
    "    if encoded_count % 100 == 0:\n",
    "        print(f\"Encoded {encoded_count} embeddings\", end=\"\\r\")\n",
    "\n",
    "print(f\"\\nTotal encoded: {encoded_count} embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Streaming from Files\n",
    "\n",
    "Process embeddings directly from binary files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test file with embeddings\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix=\".bin\") as tmp:\n",
    "    temp_filename = tmp.name\n",
    "    \n",
    "    # Write 100 embeddings of 768 dimensions each\n",
    "    for _ in range(100):\n",
    "        embedding = np.random.randint(0, 256, 768, dtype=np.uint8)\n",
    "        tmp.write(embedding.tobytes())\n",
    "\n",
    "print(f\"Created test file: {temp_filename}\")\n",
    "print(f\"File size: {os.path.getsize(temp_filename):,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process file with streaming API\n",
    "output_file = \"encoded_embeddings.txt\"\n",
    "\n",
    "encoded_list = []\n",
    "for encoded in encode_file_stream(\n",
    "    temp_filename,\n",
    "    output_file,\n",
    "    method=\"eq64\",\n",
    "    embedding_size=768\n",
    "):\n",
    "    encoded_list.append(encoded)\n",
    "\n",
    "print(f\"Processed {len(encoded_list)} embeddings\")\n",
    "print(f\"Output written to: {output_file}\")\n",
    "print(f\"First encoding: {encoded_list[0][:50]}...\")\n",
    "\n",
    "# Clean up\n",
    "os.unlink(temp_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. StreamingEncoder Context Manager\n",
    "\n",
    "Use the context manager for clean resource handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process embeddings with automatic file writing\n",
    "with StreamingEncoder(\"streaming_output.txt\", method=\"zoq64\") as encoder:\n",
    "    for i in range(50):\n",
    "        embedding = np.random.randint(0, 256, 512, dtype=np.uint8)\n",
    "        encoded = encoder.encode(embedding)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processed {encoder.count} embeddings\")\n",
    "\n",
    "print(f\"\\nTotal processed: {encoder.count} embeddings\")\n",
    "print(\"Output automatically saved to streaming_output.txt\")\n",
    "\n",
    "# Verify the output\n",
    "with open(\"streaming_output.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    print(f\"File contains {len(lines)} encoded embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Processing\n",
    "\n",
    "Process multiple embeddings at once for better performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a batch of embeddings\n",
    "batch_size = 1000\n",
    "embeddings = [\n",
    "    np.random.randint(0, 256, 384, dtype=np.uint8).tobytes()\n",
    "    for _ in range(batch_size)\n",
    "]\n",
    "\n",
    "# Time different approaches\n",
    "import time\n",
    "\n",
    "# Sequential processing\n",
    "start = time.time()\n",
    "sequential_results = [encode(emb, method=\"t8q64\", k=16) for emb in embeddings]\n",
    "sequential_time = time.time() - start\n",
    "\n",
    "# Batch processing\n",
    "start = time.time()\n",
    "batch_results = batch_encode(embeddings, method=\"t8q64\", k=16)\n",
    "batch_time = time.time() - start\n",
    "\n",
    "print(f\"Sequential processing: {sequential_time:.3f}s\")\n",
    "print(f\"Batch processing: {batch_time:.3f}s\")\n",
    "print(f\"Results match: {sequential_results == batch_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Memory-Efficient Processing Pipeline\n",
    "\n",
    "Build a complete pipeline for processing large embedding datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingPipeline:\n",
    "    \"\"\"Example pipeline for processing embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, method=\"auto\", batch_size=100):\n",
    "        self.method = method\n",
    "        self.batch_size = batch_size\n",
    "        self.stats = {\n",
    "            \"processed\": 0,\n",
    "            \"total_bytes\": 0,\n",
    "            \"encoding_lengths\": []\n",
    "        }\n",
    "    \n",
    "    def process_stream(self, embedding_source):\n",
    "        \"\"\"Process embeddings from any iterable source.\"\"\"\n",
    "        batch = []\n",
    "        \n",
    "        for embedding in embedding_source:\n",
    "            batch.append(embedding)\n",
    "            \n",
    "            if len(batch) >= self.batch_size:\n",
    "                yield from self._process_batch(batch)\n",
    "                batch = []\n",
    "        \n",
    "        # Process remaining\n",
    "        if batch:\n",
    "            yield from self._process_batch(batch)\n",
    "    \n",
    "    def _process_batch(self, batch):\n",
    "        \"\"\"Process a batch of embeddings.\"\"\"\n",
    "        encoded_batch = batch_encode(batch, method=self.method)\n",
    "        \n",
    "        for emb, encoded in zip(batch, encoded_batch):\n",
    "            self.stats[\"processed\"] += 1\n",
    "            self.stats[\"total_bytes\"] += len(emb)\n",
    "            self.stats[\"encoding_lengths\"].append(len(encoded))\n",
    "            yield encoded\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get processing statistics.\"\"\"\n",
    "        if not self.stats[\"encoding_lengths\"]:\n",
    "            return self.stats\n",
    "        \n",
    "        avg_encoding_len = np.mean(self.stats[\"encoding_lengths\"])\n",
    "        compression_ratio = self.stats[\"total_bytes\"] / sum(self.stats[\"encoding_lengths\"])\n",
    "        \n",
    "        return {\n",
    "            **self.stats,\n",
    "            \"avg_encoding_length\": avg_encoding_len,\n",
    "            \"compression_ratio\": compression_ratio\n",
    "        }\n",
    "\n",
    "# Use the pipeline\n",
    "pipeline = EmbeddingPipeline(method=\"shq64\", batch_size=50)\n",
    "\n",
    "# Process embeddings\n",
    "print(\"Processing with pipeline...\")\n",
    "results = []\n",
    "for encoded in pipeline.process_stream(embedding_generator(500, 256)):\n",
    "    results.append(encoded)\n",
    "\n",
    "# Display statistics\n",
    "stats = pipeline.get_stats()\n",
    "print(f\"\\nPipeline Statistics:\")\n",
    "print(f\"  Processed: {stats['processed']} embeddings\")\n",
    "print(f\"  Total input bytes: {stats['total_bytes']:,}\")\n",
    "print(f\"  Average encoding length: {stats['avg_encoding_length']:.1f} chars\")\n",
    "print(f\"  Compression ratio: {stats['compression_ratio']:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Streaming with Progress Tracking\n",
    "\n",
    "Add progress tracking for long-running operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def process_with_progress(n_embeddings=10000):\n",
    "    \"\"\"Process embeddings with a progress bar.\"\"\"\n",
    "    \n",
    "    # Create embedding source\n",
    "    embeddings = embedding_generator(n_embeddings, 512)\n",
    "    \n",
    "    # Process with progress tracking\n",
    "    encoded_embeddings = []\n",
    "    \n",
    "    with tqdm(total=n_embeddings, desc=\"Encoding embeddings\") as pbar:\n",
    "        for encoded in encode_stream(embeddings, method=\"eq64\", batch_size=100):\n",
    "            encoded_embeddings.append(encoded)\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return encoded_embeddings\n",
    "\n",
    "# Note: If tqdm is not installed, you can install it with: pip install tqdm\n",
    "try:\n",
    "    encoded = process_with_progress(1000)\n",
    "    print(f\"\\nEncoded {len(encoded)} embeddings successfully!\")\n",
    "except ImportError:\n",
    "    print(\"Install tqdm for progress bars: pip install tqdm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up generated files\n",
    "for filename in [\"encoded_embeddings.txt\", \"streaming_output.txt\"]:\n",
    "    if os.path.exists(filename):\n",
    "        os.unlink(filename)\n",
    "        print(f\"Removed {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "1. Streaming from generators for memory-efficient processing\n",
    "2. Processing embeddings directly from files\n",
    "3. Using the StreamingEncoder context manager\n",
    "4. Batch processing for better performance\n",
    "5. Building complete processing pipelines\n",
    "6. Adding progress tracking\n",
    "\n",
    "Key benefits of the streaming API:\n",
    "- **Memory efficient**: Process datasets larger than RAM\n",
    "- **Flexible**: Works with any iterable source\n",
    "- **Fast**: Batch processing improves throughput\n",
    "- **Clean**: Context managers handle resources automatically\n",
    "\n",
    "Next, check out the LangChain integration notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}