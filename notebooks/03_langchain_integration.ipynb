{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Integration\n",
    "\n",
    "This notebook demonstrates how to use uubed with LangChain for enhanced embedding management in RAG applications.\n",
    "\n",
    "**Note**: This notebook requires LangChain to be installed:\n",
    "```bash\n",
    "pip install langchain openai chromadb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if required packages are available\n",
    "try:\n",
    "    import langchain\n",
    "    print(f\"LangChain version: {langchain.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Please install LangChain: pip install langchain\")\n",
    "\n",
    "from uubed.integrations.langchain import (\n",
    "    UubedEncoder,\n",
    "    UubedEmbeddings,\n",
    "    UubedDocumentProcessor,\n",
    "    create_uubed_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Document Encoding\n",
    "\n",
    "Let's start by encoding document embeddings with uubed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "\n",
    "# Create sample documents\n",
    "documents = [\n",
    "    Document(page_content=\"The quick brown fox jumps over the lazy dog.\", metadata={\"id\": 1}),\n",
    "    Document(page_content=\"Python is a versatile programming language.\", metadata={\"id\": 2}),\n",
    "    Document(page_content=\"Machine learning transforms data into insights.\", metadata={\"id\": 3}),\n",
    "]\n",
    "\n",
    "# Simulate embeddings (in practice, these come from an embedding model)\n",
    "embeddings = [\n",
    "    np.random.randn(768).tolist(),  # Normalized embeddings\n",
    "    np.random.randn(768).tolist(),\n",
    "    np.random.randn(768).tolist(),\n",
    "]\n",
    "\n",
    "# Create encoder and encode documents\n",
    "encoder = UubedEncoder(method=\"shq64\")\n",
    "encoded_docs = encoder.encode_documents(documents, embeddings)\n",
    "\n",
    "# Check the results\n",
    "for doc in encoded_docs:\n",
    "    print(f\"Document {doc.metadata['id']}: {doc.page_content[:50]}...\")\n",
    "    print(f\"  Encoded: {doc.metadata['uubed_encoded']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using UubedEmbeddings Wrapper\n",
    "\n",
    "Wrap any LangChain embedding model to add uubed encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mock embedding class for demonstration\n",
    "class MockEmbeddings:\n",
    "    \"\"\"Mock embeddings for demonstration (replace with OpenAIEmbeddings, etc.)\"\"\"\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        # Simulate embedding generation\n",
    "        return [np.random.randn(384).tolist() for _ in texts]\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        # Simulate query embedding\n",
    "        return np.random.randn(384).tolist()\n",
    "    \n",
    "    async def aembed_documents(self, texts):\n",
    "        return self.embed_documents(texts)\n",
    "    \n",
    "    async def aembed_query(self, text):\n",
    "        return self.embed_query(text)\n",
    "\n",
    "# Create base embeddings (in practice: OpenAIEmbeddings, CohereEmbeddings, etc.)\n",
    "base_embeddings = MockEmbeddings()\n",
    "\n",
    "# Wrap with uubed\n",
    "uubed_embeddings = UubedEmbeddings(\n",
    "    base_embeddings,\n",
    "    method=\"t8q64\",\n",
    "    return_encoded=True,  # Return encoded strings instead of raw embeddings\n",
    "    k=16  # Parameter for t8q64 method\n",
    ")\n",
    "\n",
    "# Use like any other embedding model\n",
    "texts = [\n",
    "    \"Hello, world!\",\n",
    "    \"Machine learning is fascinating.\",\n",
    "    \"LangChain makes LLM apps easy.\"\n",
    "]\n",
    "\n",
    "encoded_embeddings = uubed_embeddings.embed_documents(texts)\n",
    "print(\"Encoded embeddings:\")\n",
    "for text, encoded in zip(texts, encoded_embeddings):\n",
    "    print(f\"  '{text}' -> {encoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Processing Pipeline\n",
    "\n",
    "Use UubedDocumentProcessor in your document ingestion pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document processor\n",
    "processor = UubedDocumentProcessor(\n",
    "    embeddings=base_embeddings,\n",
    "    method=\"zoq64\",\n",
    "    batch_size=2  # Process 2 documents at a time\n",
    ")\n",
    "\n",
    "# Create more documents\n",
    "large_doc_set = [\n",
    "    Document(page_content=f\"Document {i}: This is test content for document number {i}.\", \n",
    "             metadata={\"doc_id\": i, \"source\": \"test\"})\n",
    "    for i in range(10)\n",
    "]\n",
    "\n",
    "# Process documents\n",
    "processed_docs = processor.process(large_doc_set)\n",
    "\n",
    "print(f\"Processed {len(processed_docs)} documents\")\n",
    "print(\"\\nSample processed document:\")\n",
    "print(f\"  Content: {processed_docs[0].page_content}\")\n",
    "print(f\"  Metadata: {processed_docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Integration with Vector Stores\n",
    "\n",
    "Here's how to use uubed with vector stores for enhanced retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock vector store for demonstration\n",
    "class MockVectorStore:\n",
    "    \"\"\"Mock vector store (replace with Chroma, Pinecone, etc.)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "        self.embedding_function = None\n",
    "    \n",
    "    def add_documents(self, documents):\n",
    "        self.documents.extend(documents)\n",
    "        return [f\"id_{i}\" for i in range(len(documents))]\n",
    "    \n",
    "    def as_retriever(self, **kwargs):\n",
    "        return self\n",
    "    \n",
    "    def get_relevant_documents(self, query):\n",
    "        # Simple mock retrieval\n",
    "        return self.documents[:2] if self.documents else []\n",
    "\n",
    "# Create vector store and retriever\n",
    "vectorstore = MockVectorStore()\n",
    "\n",
    "# Add processed documents\n",
    "doc_ids = vectorstore.add_documents(processed_docs)\n",
    "print(f\"Added {len(doc_ids)} documents to vector store\")\n",
    "\n",
    "# Create uubed-enhanced retriever\n",
    "retriever = create_uubed_retriever(\n",
    "    vectorstore=vectorstore,\n",
    "    embeddings=base_embeddings,\n",
    "    method=\"shq64\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "# Use the retriever\n",
    "query = \"Tell me about document processing\"\n",
    "results = retriever.get_relevant_documents(query)\n",
    "print(f\"\\nRetrieved {len(results)} documents for query: '{query}'\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"  {i+1}. {doc.page_content[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete RAG Pipeline Example\n",
    "\n",
    "Here's a complete example showing how uubed fits into a RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UubedRAGPipeline:\n",
    "    \"\"\"Example RAG pipeline with uubed encoding.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model, vector_store, encoding_method=\"auto\"):\n",
    "        # Wrap embeddings with uubed\n",
    "        self.embeddings = UubedEmbeddings(\n",
    "            embedding_model,\n",
    "            method=encoding_method,\n",
    "            return_encoded=False  # Keep raw embeddings for vector search\n",
    "        )\n",
    "        \n",
    "        # Document processor for ingestion\n",
    "        self.processor = UubedDocumentProcessor(\n",
    "            self.embeddings,\n",
    "            method=encoding_method\n",
    "        )\n",
    "        \n",
    "        # Vector store for retrieval\n",
    "        self.vector_store = vector_store\n",
    "        self.vector_store.embedding_function = self.embeddings\n",
    "    \n",
    "    def ingest_documents(self, documents):\n",
    "        \"\"\"Ingest documents with uubed encoding.\"\"\"\n",
    "        # Process documents (adds encoded embeddings to metadata)\n",
    "        processed = self.processor.process(documents)\n",
    "        \n",
    "        # Add to vector store\n",
    "        ids = self.vector_store.add_documents(processed)\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def search(self, query, k=5):\n",
    "        \"\"\"Search for relevant documents.\"\"\"\n",
    "        # The query will be automatically encoded by UubedEmbeddings\n",
    "        retriever = self.vector_store.as_retriever(search_kwargs={\"k\": k})\n",
    "        return retriever.get_relevant_documents(query)\n",
    "    \n",
    "    def get_encoding_stats(self):\n",
    "        \"\"\"Get statistics about encoded documents.\"\"\"\n",
    "        encoded_lengths = []\n",
    "        for doc in self.vector_store.documents:\n",
    "            if \"uubed_encoded\" in doc.metadata:\n",
    "                encoded_lengths.append(len(doc.metadata[\"uubed_encoded\"]))\n",
    "        \n",
    "        if encoded_lengths:\n",
    "            return {\n",
    "                \"total_documents\": len(self.vector_store.documents),\n",
    "                \"encoded_documents\": len(encoded_lengths),\n",
    "                \"avg_encoding_length\": np.mean(encoded_lengths),\n",
    "                \"encoding_method\": self.processor.encoder.method\n",
    "            }\n",
    "        return {\"total_documents\": 0}\n",
    "\n",
    "# Create and use the pipeline\n",
    "pipeline = UubedRAGPipeline(\n",
    "    embedding_model=MockEmbeddings(),\n",
    "    vector_store=MockVectorStore(),\n",
    "    encoding_method=\"shq64\"\n",
    ")\n",
    "\n",
    "# Ingest documents\n",
    "sample_docs = [\n",
    "    Document(page_content=\"RAG combines retrieval with generation for better AI responses.\"),\n",
    "    Document(page_content=\"Embeddings capture semantic meaning in vector space.\"),\n",
    "    Document(page_content=\"uubed provides position-safe encoding for embeddings.\"),\n",
    "]\n",
    "\n",
    "doc_ids = pipeline.ingest_documents(sample_docs)\n",
    "print(f\"Ingested {len(doc_ids)} documents\")\n",
    "\n",
    "# Search\n",
    "results = pipeline.search(\"How do embeddings work?\", k=2)\n",
    "print(\"\\nSearch results:\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"  {i+1}. {doc.page_content}\")\n",
    "    if \"uubed_encoded\" in doc.metadata:\n",
    "        print(f\"     Encoded: {doc.metadata['uubed_encoded']}\")\n",
    "\n",
    "# Get statistics\n",
    "stats = pipeline.get_encoding_stats()\n",
    "print(f\"\\nPipeline statistics: {stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced: Custom Encoding Strategies\n",
    "\n",
    "Choose encoding methods based on your use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_encoding_method(embedding_dim, use_case=\"general\"):\n",
    "    \"\"\"Select optimal encoding method based on use case.\"\"\"\n",
    "    \n",
    "    if use_case == \"exact_match\":\n",
    "        # Need lossless encoding for exact matching\n",
    "        return \"eq64\", {}\n",
    "    \n",
    "    elif use_case == \"similarity_search\":\n",
    "        # Compact similarity-preserving encoding\n",
    "        return \"shq64\", {\"planes\": min(64, embedding_dim // 4)}\n",
    "    \n",
    "    elif use_case == \"sparse\":\n",
    "        # Sparse representation for high-dimensional embeddings\n",
    "        k = min(32, embedding_dim // 10)\n",
    "        return \"t8q64\", {\"k\": k}\n",
    "    \n",
    "    elif use_case == \"spatial\":\n",
    "        # Spatial locality for range queries\n",
    "        return \"zoq64\", {}\n",
    "    \n",
    "    else:\n",
    "        # Auto-select based on dimensions\n",
    "        return \"auto\", {}\n",
    "\n",
    "# Test different strategies\n",
    "use_cases = [\"exact_match\", \"similarity_search\", \"sparse\", \"spatial\"]\n",
    "embedding_dims = [384, 768, 1536]\n",
    "\n",
    "print(\"Encoding strategy recommendations:\")\n",
    "print(\"-\" * 60)\n",
    "for dim in embedding_dims:\n",
    "    print(f\"\\nEmbedding dimension: {dim}\")\n",
    "    for use_case in use_cases:\n",
    "        method, params = select_encoding_method(dim, use_case)\n",
    "        print(f\"  {use_case:20s}: {method} {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "1. **Basic encoding**: Adding uubed-encoded embeddings to document metadata\n",
    "2. **Embedding wrapper**: Using UubedEmbeddings to wrap any LangChain embedding model\n",
    "3. **Document processing**: Batch processing documents with automatic encoding\n",
    "4. **Vector store integration**: Creating enhanced retrievers with uubed\n",
    "5. **Complete RAG pipeline**: Full example of uubed in a RAG application\n",
    "6. **Encoding strategies**: Selecting optimal encoding methods for different use cases\n",
    "\n",
    "Key benefits of using uubed with LangChain:\n",
    "- **Position-safe**: Prevents substring pollution in search\n",
    "- **Flexible**: Works with any embedding model and vector store\n",
    "- **Efficient**: Multiple encoding methods for different needs\n",
    "- **Easy integration**: Drop-in components for existing pipelines\n",
    "\n",
    "Next steps:\n",
    "- Try with real embedding models (OpenAI, Cohere, etc.)\n",
    "- Test with production vector stores (Chroma, Pinecone, Weaviate)\n",
    "- Experiment with different encoding methods for your use case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}